{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI E-25      \n",
    "## Working with Image Data  \n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction   \n",
    "\n",
    "This lesson will familarize you with the basic concepts of working with image data and some statistical properties of images. Some key points are of this lesson are:     \n",
    "1. Discrete pixel structure of digital images.    \n",
    "2. Representation of color and gray scale images.   \n",
    "3. Intensity distribution of image data.\n",
    "4. Equalizing intensity distributions and improving contrast. \n",
    "5. Resizing images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Scikit-Learn Image:** In this lesson we will be using the Scikit-Learn Image package. This package provides many commonly used computer vision algorithms using a consistent Python API. This package follows the API conventions of Scikit-Learn, enabling the application of a rich library of machine learning algorithms. There is excellent documentation available for the [Scikit-Learn Image package](https://scikit-image.org/docs/stable/index.html). You may wish to start by reading through the [User Guide](https://scikit-image.org/docs/stable/user_guide.html). Examples of what you can do with Scikit-Learn Image package can be seen in the [Gallery](https://scikit-image.org/docs/stable/auto_examples/index.html).    \n",
    "\n",
    "> **Installation:** To run the code in this notebook you will need to install Scikit-Learn Image. Follow the [installation directions](https://scikit-image.org/docs/stable/install.html) for your operating system.      \n",
    "\n",
    "> **Scikit-Learn Image and Numpy:** Like all Scikit packages this one is built on [Numpy](https://numpy.org/). If you are not very familiar with Numpy you can find Tutorials [here](https://numpy.org/numpy-tutorials/). A tutorial on using Numpy with Scikit-learn image data objects can be found [here](https://scikit-image.org/docs/stable/user_guide/numpy_images.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "To get started with this lesson, execute the code in the cell below to import the packages you will need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage \n",
    "from skimage import data\n",
    "from skimage.filters.rank import equalize, threshold\n",
    "import skimage.filters as skfilters\n",
    "from skimage import exposure\n",
    "from skimage.morphology import disk, square\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a Color Image Object\n",
    "\n",
    "The code in the cell below loads a color image of a human retina, prints the data types and dimensions of the image object, and displays the image. The image is displayed by [matplotlib.pyplot.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html). Execute the code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_image = data.retina()\n",
    "print('The image object is ' + str(type(retina_image)))\n",
    "print('The pixel values are of type ' + str(type(retina_image[0,0,0])))\n",
    "print('Shape of image object = ' + str(retina_image.shape))\n",
    "fig, ax = plt.subplots( figsize=(6, 6))\n",
    "_=ax.imshow(retina_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image object has 3-dimensions, the two spatial dimensions and the 3 color channels. Examine this image noticing the wide variation in color and intensity. Notice also that the illumination of the retina does not appear uniform, resulting in a bright spot on the left and a darker region on the right.  \n",
    "\n",
    "> **Exercise 1-1:** Complete the function in the cell below to display the 3 color channels of the image and the original image in a 2x2 array using the [matplotlib.pyplot.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) function. The color channels are in red, green, blue order and should be displayed as gray scale using the `cmap=plt.get_cmap('gray')` argument. Your function should label the channels and the original image. Execute your function and examine the results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3_color_channels(img):\n",
    "    '''Function plots the three color channels of the image along with the complete image'''\n",
    "    fig, ax = plt.subplots(2,2, figsize=(12, 12))\n",
    "    ax = ax.flatten()\n",
    "    ## Put you code below\n",
    "    \n",
    "    \n",
    "plot_3_color_channels(retina_image)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the intensity (brightness) of the color channels and answer these questions:    \n",
    "> 1. Which channel has the greatest intensity, and does this make sense given the image?      \n",
    "> 2. Is it likely that the saturation of the red channel arises as an artifact of the illumination spot on the left of the retina image?      \n",
    "> **End of exercise.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1. \n",
    "> 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with digital images it is always important to keep in mind the discrete nature of the samples. To demonstrate the discrete nature of a digital image you can visualize a 100 pixel, or $10 \\times 10$, sample from the larger image by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3_color_channels(retina_image[600:610,300:310,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the discrete nature in each of the three color channels and the color image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Properties of an Image  \n",
    "\n",
    "The next question is, what is the distribution of pixel intensities in the 3 color channels of the image? Histograms and cumulative density functions are used to analyze these distributions. The code in the cell below plots the histograms of the 3 color channels along with their cumulative distributions. Execute this code and examine the results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_image_distributions(img):\n",
    "    '''Function plots histograms of the three color channels of the image along \n",
    "    with the cumulative distributions'''\n",
    "    fig, ax = plt.subplots(2,2, figsize=(12, 12))\n",
    "    ax = ax.flatten()\n",
    "    titles=['Red','Green','Blue']\n",
    "    for i in range(3):\n",
    "        ax[i].hist(img[:,:,i].flatten(), bins=50, density=True, color=titles[i], alpha=0.3)\n",
    "        ax[i].set_title(titles[i])\n",
    "        ax[i].set_xlabel('Pixel value')\n",
    "        ax[i].set_ylabel('Density')\n",
    "        ax[3].hist(img[:,:,i].flatten(), bins=50, density=True, cumulative=True, color=titles[i], histtype='step')\n",
    "    ax[3].set_title('Cumulative distributions')  \n",
    "    ax[3].set_xlabel('Pixel value')\n",
    "    ax[3].set_ylabel('Cumulative density')  \n",
    "    \n",
    "plot_image_distributions(retina_image)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several properties of the distribution of the pixel values which are important:     \n",
    "1. The distribution of the intensity for of the red channel has clearly higher values than the other channels.    \n",
    "2. A significant fraction of pixel values have 0 intensity for all 3 color channels. These pixels are primarily black background around the retina, but may also represent the dark pupil spot in the center of the retina.   \n",
    "3. A few red channel pixels have the maximum value of 255. The red intensity of these pixels is said to be saturated.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensity Equalization    \n",
    "\n",
    "Our next question to address is what is the ideal distribution of the intensity values of an image? A useful, and obviously answer, is that we want the pixel values over the full range of possible values. For unsigned integer values, {0,255}. Further, the distribution of pixel values should be uniform. For the $n=256$ unsigned integer values the **probability mass function**, or **PMF**, of the $ith$ value is:     \n",
    "\n",
    "$$p(i) = \\frac{1}{n}$$      \n",
    "\n",
    "And the **cumulative density function**, or **CDF**, of the uniform distribution at the $ith$ value is:   \n",
    "\n",
    "$$CDF(i) = \\sum_0^i \\frac{1}{n}$$  \n",
    "\n",
    "We can visualize an example of a gray-scale image of unsigned integers on the range {0,255} with random uniform distributed pixel values. The code in the cell below forms a gray-scale image randomly sampled uniform distributed pixel values and displays the result.               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_image = np.multiply((nr.uniform(low=0.0, high=255.0, size=retina_image.shape[0]*retina_image.shape[1])), 255).reshape((retina_image.shape[0], retina_image.shape[1]))\n",
    "random_image = random_image.astype(np.uint8)\n",
    "print(random_image.shape)\n",
    "fig, ax = plt.subplots( figsize=(6, 6))\n",
    "_=ax.imshow(random_image, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the distribution of the pixel values of this image execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gray_scale_distribution(img):\n",
    "    '''Function plots histograms a gray scale image along \n",
    "    with the cumulative distribution'''\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12, 5))\n",
    "    ax[0].hist(img.flatten(), bins=50, density=True, alpha=0.3)\n",
    "    ax[0].set_title('Histogram of image')\n",
    "    ax[0].set_xlabel('Pixel value')\n",
    "    ax[0].set_ylabel('Density')\n",
    "    ax[1].hist(img.flatten(), bins=50, density=True, cumulative=True, histtype='step')\n",
    "    ax[1].set_title('Cumulative distribution of image')  \n",
    "    ax[1].set_xlabel('Pixel value')\n",
    "    ax[1].set_ylabel('Cumulative density') \n",
    "    plt.show()\n",
    "\n",
    "plot_gray_scale_distribution(random_image)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-2:** To compare the pixel value distribution of the retina image to the ideal values do the following:   \n",
    "> 1. Create a gray-scale image object named `retina_gray_scale` using the [skimage.color.rgb2gray](https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_rgb_to_gray.html) function. \n",
    "> 2. Print the dimensions of the image object.  \n",
    "> 3. Display the gray-scale image. Make sure the image is large enough to see the details.   \n",
    "> 4. Plot the distribution of the pixel values of the gray-scale image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grayscale(img):\n",
    "    fig, ax = plt.subplots( figsize=(6, 6))\n",
    "    _=ax.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "\n",
    "## Put you code below    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_image.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the distribution plots. How would you describe these results with respect to the ideal distribution? How do you think the range of pixel values limit the contrast of the image?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contrast** of an image is range between the minimum and maximum pixel values of an image. The larger the range of values the more distinctive the differences in the image will be. To improve the contrast in an image we need to **equalize** the pixel values over the maximum of the range. The goal is to find a transformation that stretches the pixel values into a uniform distribution. This process is know as **histogram equalization**.      \n",
    "\n",
    "Histogram equalization can be performed in a number of ways. The obvious algorithm is global histogram equalization. The pixel values are transformed to equalize the histogram across the entire image. However, if illumination is inconsistent across the image, global equalization will not be optimal. An alternative is to perform local histogram equalization over small areas of the image. This method is known as **adaptive histogram equalization**. Adaptive equalization can compensate for uneven illumination across the image.           \n",
    "\n",
    "> **Exercise 1-3:** You will now apply both common types of histogram equalization to the gray-scale retina image. Use both the [sklearn.exposure.equalize_hist](https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html) function and the [sklearn.exposure.equalize_adapthist](https://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.equalize_adapthist) function. Create and execute a function which does the following:    \n",
    "> 1. Executes the equalization function passed as an argument. Pass the function name as a string using the Python [eval](https://www.w3schools.com/python/ref_func_eval.asp) function.  \n",
    "> 2. Display the equalized gray-scale image using the `plot_grayscale()` function.   \n",
    "> 3. Plot the distribution of the pixel values of the equalized gray-scale image using the `plot_gray_scale_distribution()` function.    \n",
    "> \n",
    "> Execute your function for the two equalization algorithms. You can do this by iterating over a list of the function names. Print a line indicating which function is being executed. Save the results in a Numpy object named `retina_gray_scale_equalized`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_equalize(img, func): \n",
    "    img_equalized = np.multiply(eval(func)(img), 255).astype(np.uint8)\n",
    "    plot_grayscale(np.divide(img_equalized, 255.0))\n",
    "    plot_gray_scale_distribution(np.divide(img_equalized, 255.0))    \n",
    "    return img_equalized\n",
    "\n",
    "## Put you code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:  \n",
    "> 1. Compare the unequalized and equalized images. What aspects of the of the images are more apparent with the improved contrasted.  \n",
    "> 2. Compare the distributions of pixel values between the unequalized image, the random uniformly distributed image, and equalized images (2). Which of these histograms look the most similar and what does this tell you about the contrast of the image. \n",
    "> 3. Does the difference in the distribution between the locally equalized image and the globally equalized image make sense and why?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1. \n",
    "> 2. \n",
    "> 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-4:** Equalization of multi-channel images, including color images, adds complications. As you have already seen, the distribution of pixel values for each of the channels is likely to be quite different. Therefore, each channel must be equalized independently.  You will now perform this operation on the color retina image.    \n",
    "> The equalization is done by iterating over the color channels. Use the [sklearn.exposure.equalize_adapthist](https://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.equalize_adapthist) function.  \n",
    "> Finally, display the images for each of the color channels and the complete image using your `plot_3_color_channels()` function and the `plot_image_distributions()` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer the answer the following questions:      \n",
    "> 1. Did the histogram equalization achieve the goal of improving the contrast of the image both in the color channels and for the 3-channel color image, and why?   \n",
    "> 2. Given the use of the locally adapted histogram equalization algorithm, does the distribution of the pixel values in the 3 channels of the equalized image make sense, and why?    \n",
    "> 3. What is the evidence of saturation of the red color channel after equalization?    \n",
    "> 4. Does the change in color of the 3-channel color image make sense given the histogram equalization, and why?    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1. \n",
    "> 2.    \n",
    "> 3.   \n",
    "> 4.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast improvement is such an important data preparation step for computer vision that many algorithms have been proposed. One approach is to use rank statistics over a small region or local region of the image. The [skimage.filters.rank.equalize](https://scikit-image.org/docs/dev/api/skimage.filters.rank.html#skimage.filters.rank.equalize) function implements just such an algorithm. Execute the code in the cell below to see the effect this algorithm has on the gray-scale retina image. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_local_equalized = equalize(np.multiply(retina_gray_scale, 255).astype(np.uint8), selem=disk(9))\n",
    "plot_grayscale(retina_local_equalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This locally equalized image shows considerably more detail than the global histogram equalization or adaptive histogram equalization methods. But, is this what we really want? In some cases yes. If fine details like texture are important to the computer vision solution, this equalization would be preferred. However, too much detail might lead to unnecessary complexity if the goal was to identify major structural elements of the image. In summary, the correct preprocessing for an image depends on the other algorithms one intends to apply. } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Images  \n",
    "\n",
    "Many computer vision algorithms operate on binary images. Primarily these methods are in the category of **morphology**, which we will explore later. A binary image has only two values, $\\{positive, negative \\}$ or $\\{ 1, 0 \\}$.   \n",
    "\n",
    "> **Exercise 1-5:** You will create a function named `transform2binry()` to convert either a 3-channel color image or gray scale image to a integer binary image, $\\{ 1, 0 \\}$, given a threshold value in the range $0 \\le threshold \\le 1$ as an argument. The function must do the following:    \n",
    "> 1. If the image is multi-channel, convert it to gray-scale.     \n",
    "> 2. Transform the threshold value to the fraction of the range of the pixel values. Print the transformed threshold value.    \n",
    "> 3. Apply the threshold to the gray-scale pixel values and return the binary images.     \n",
    "> 4. Execute your function on the equalized color retina image, print the dimensions of the binary image, and display the image, using a threshold value of 0.37.  \n",
    "> 5. Execute your function on the locally equalized color retina image, print the dimensions of the binary image, and display the image, using a threshold value of 0.37.      \n",
    "> 6. Execute your function on the equalized color retina and display the image, using a threshold value of 0.37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the image and answer the following questions.  \n",
    "> 1. Does the binary image created from the unequalized color image capture key aspects of the retina and why?     \n",
    "> 2. Compare the binary images created from the equalized color image and the equalized gray scale image. Is there any difference, and is this the result you would expect?  \n",
    "> **End of exercise.**      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.     \n",
    "> 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the foregoing exercise, the threshold for the decision classifying pixel values as true or false, $\\{ 0, 1 \\}$ was set manually by trial and error. There are numerous algorithms which have been devised for finding thresholds. In general, these algorithms attempt to find an optimal threshold using various measures. Ideally, these algorithms search for a low frequency point in the pixel value histograms which can be used to divide the values.     \n",
    "\n",
    "**Exercise 1-6:** We can create a binary image using one of the many established algorithms to compute a threshold. In this cease [Otsu's threshold algorithm](https://scikit-image.org/docs/dev/api/skimage.filters.html?highlight=threshold_otsu#skimage.filters.threshold_otsu). Use this function to find a threshold, apply the threshold to the the equalized gray-scale image to compute a binary image, and plot the result.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How does this binary image compare the to ones computed with the threshold found by trail-and-error, and why?       \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion of Images  \n",
    "\n",
    "For some machine vision algorithms it is easier or more effective to work with the **inverse image** or **negative** of the image. The concept is simple. Pixel values are generally restricted to a range like $\\{ 0 - 255 \\}$ for unsigned integer representation or $\\{ 0.0 - 1.0 \\}$ for a floating point image. The given an intensity $P_{i,j}$ of the $ij$th pixel, the inverted intensity, $I_{i,j}$, is then:\n",
    "\n",
    "$$I_{i,j} = max\\big[ P \\big] - P_{i,j}$$\n",
    "\n",
    "Where, $max\\big[ P \\big]$ is the largest value the representation of the image allows, typically 255 or 1.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-7:** You will now write a function named `invert_image` that will perform image inversion on both 3-channel and gray-scale images. Make sure you find the correct maximum value for the data type of the image.     \n",
    "> \n",
    "> Now, apply your function to the original color retina image and display the image along with the distribution plot.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below\n",
    "\n",
    "np.unique(arr, return_inverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, apply your function to the adaptive histogram equalized gray-scale retina image and display the image along with the distribution plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> 1. Compare the distribution of the pixel values for the three color channels of the inverted image with the distributions for the original image. Do the distributions for the inverted image make sense given the original values and why? \n",
    "> 2. Do you think the color of the 3-channel inverted image is correct and why?   \n",
    "> 3. The difference in pixel value distributions between the inverted gray scale and original adaptive histogram distributions are subtle. What key difference can you identify?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.    \n",
    "> 2.\n",
    "> 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and resizing images      \n",
    "\n",
    "For many computer vision processes the dimensions of an image must be transformed. We have already explored removing the multi-channel color dimension from an image to form the gray-scale image. Now, we will investigate transformation the pixel row and column dimensions of an image. There are two options:       \n",
    "1. **Down-sample:** A down sampled image has a reduced number of pixels. If the multiple between the pixel count of the original image and the down-sampled image an even number, sampled pixel values are used. Otherwise, interpolation is required for arbitrary multiples. Inevitably, down-sampling will reduce the resolution of the image, and fine details will be lost.                    \n",
    "2. **Up-sample:** The number of samples can be increased by interpolation between the pixel values. The interpolated values fill in the values of the new pixel. If the pixel count of the up-sampled image is not an even multiple of the original image most of the values will be interpolated. While up-sampling can increase the number number of the pixels, this process cannot increase the resolution of an image.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-8:** You will now resize the adaptive histogram equalized gray-scale image. Using the [skimage.transform.resize](https://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize) function, do the following:      \n",
    "> 1. Down-sample the image to dimension $(64,64)$. Print the dimensions and display the resulting image.      \n",
    "> 2. Up-sample the down-sampled image to dimension $(1024,1024)$. Print the dimensions and display the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## down sample the image\n",
    "## Put you code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Up-sample the image\n",
    "## Put you code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the changes in resolution of the down-sampled and up-sampled images.  \n",
    "> 1. How is the reduction in resolution of the $(64,64)$ image exhibited? \n",
    "> 2. Does up-sampling to $(1024,1024)$ restore the resolution of the image or simple blur the 'pixelation' visible in the $(64,64)$ image? \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.  \n",
    "> 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and Aliasing in Images     \n",
    "\n",
    "As should be clear from the foregoing, the digital images we work with for computer vision are discretely sampled in the 2-dimensional plane. The discrete pixel values are $v_{\\mathbf{x}}$ are the result of this sampling. This discrete sampling limits the **spatial resolution** which can be captured in the image. If the samples are spaced too far apart, [aliasing](https://en.wikipedia.org/wiki/Aliasing) will occur. For sinusoidal components of the image the [Nyquist Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) the sampling frequency must be at least 2 time the frequency of this component. The sampling rate of 2 times the highest frequency component is known as the **Nyquist Frequency**. Sampling below the Nyquist frequency leads to aliasing.            \n",
    "\n",
    "We can demonstrate the concept of aliasing with a simple example. The example is based on an initial image and three down-sampled versions:    \n",
    "1. The initial image has diagonal slashes with sinusoidal amplitudes and dimension $(256,256)$.  \n",
    "2. The image is down-sampled to dimension $(256, 256)$. \n",
    "3. The image is down-sampled to dimension $(128, 128)$. \n",
    "2. The image is down-sampled to dimension $(64, 64)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "x = np.arange(0, dim*dim, 1)\n",
    "sin_image = 1.0 - 0.5 * np.sin(np.divide(x, math.pi)).reshape((dim,dim))\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(12, 12))\n",
    "ax = ax.flatten()\n",
    "for i,dim in enumerate([256,128,64,32]):\n",
    "    sampled_image = resize(sin_image, (dim,dim))\n",
    "    _=ax[i].imshow(sampled_image, cmap=plt.get_cmap('gray'))\n",
    "    _=ax[i].set_title('Dimension = (' + str(dim) + ',' + str(dim) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the images above and notice the following:  \n",
    "1. The $(128, 128)$ down-sampled image retains the characteristics of the initial image. Look carefully, you can see a slight blurring.      \n",
    "2. The $(64, 64)$ down-sampled image retains the sinusoidal slash structure. Coarse pixelation is now quite evident and the sampling is very close to the Nyquist frequency.     \n",
    "3. The $(32, 32)$ down-sampled image does not resemble the initial image at all, exhibiting significant aliasing. Run your eye side to side and up and down on the image. You may see patterns that are not representative of the original image. Such false patterns are a common artifact arising from aliasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can aliasing be prevented? A filter can remove the high frequency components of the image which would lead to the aliasing. A common approach is to use a Gaussian filter. This filter removes high frequency components and has the effect of blurring the image.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-9:** You will now investigate how filtering can be applied to prevent aliasing. The [skimage.transform.resize](https://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize) function applies a Gaussian filter to prevent aliasing by default. The standard devision of the Gaussian filter, or filter span, can be set to adjust the bandwidth of the filter.    \n",
    "> In this exercise you will resample the adaptive histogram equalized gray-scale retina image using the skimage.transform.resize function with the `anti_aliasing=False` argument. You will use the [skimage.filters.gaussian](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.gaussian) to limit the bandwidth of the image. Do the following to compare the results of different filter bandwidths:   \n",
    "> 1. Computer a scale factor, $sf = \\sqrt{\\frac{original\\ dimension}{reduced\\ dimension}}$.\n",
    "> 2. Apply the Gaussian filter with $sigma = mutltiplier * sf$ for multiplier in the range 0 to 4 and resize the image to $(64,64)$ pixels.\n",
    "> 3. For each value of sigma display the image with a title indicating the value of sigma. You may find interpretation easier if you plot the images on a $3 \\times 2$ grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Put you code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions.   \n",
    "> 1. How does the aliasing change with increasing sigma, decreasing bandwidth? Is this the behavior you expect and why?  \n",
    "> 2. How does the blurring of the image change with increasing sigma, decreasing bandwidth? Is this the behavior you expect and why?\n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.    \n",
    "> 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2021, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
